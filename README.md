## DataEngineering_Capstone_Project
This is the first Capstone project as part of Data Engineering Bootcamp at Analytix Labs

## Business Objectives:
The project requires to perform Data Engineering steps for Analyzing Employees level Data of an Orginzation.

Performing steps viz.
- import the CSVs into a SQL database
- import to HDFS/Hive, 
- perform analysis using Hive/Impala/Spark/SparkML using the data and create pipelines.

## Technology Stack Used:

As a part of technology stack you are required to work on below technology Stack.

- MySQL (to create database).
- Linux Commands (To access and perform Sqoop Commands, create/delete directories)
- Sqoop (Transfer data from MySQL Server to HDFS/Hive)
- HDFS (to store the data)
- Hive (to create database)
- Impala (to perform the EDA)
- SparkSQL (to perform the EDA)
- SparkML (to perform model building)


## Data Used

### employees.csv
Grain: Employee, Data regarding Employees, first_name, last_name, hire_date, last_date, employee_rating e.t.c.

### dept_emp.csv
Grain: Employee, Data Mapping departmet no and employee no

### dept_manager.csv
Grain: Manager, Data Mapping Department with Manager

### departments.csv
Grain: Employee, Data Mapping associated with 


### dept_manager.csv
Grain: Manager, Mapping Mangaer dept_no with manager emp_no


### salaries.csv
Grain: Employee, Mapping employee_id with the respective salaries assciated



## Reference ERD diagram of the Database strcuture


![RDD-Diagram](https://user-images.githubusercontent.com/101033996/169640558-c1f70547-06a2-4b21-99cc-5ffa8d4f6154.png)


## Objective of the Project:
As part of this project, you are required to work on:

- Create data model as per your understanding from the data (you are required include tables names, relation between tables, column names, data types, primary & foreign keys etc.) Tip: You can create ER diagram either in EXCEL or using the link https://www.quickdatabasediagrams.com/ (Preferably in this app)
- Create database & tables in MySQL server as per the above ER Diagram
- Create Sqoop job to transfer the data from MySQL to HDFS (Data required to store in Parque/Avro/Json format)
- Create database in Hive as per the above ER Diagram and load the data into Hive tables
- Work on Exploratory data analysis as per the analysis requirement using Impala and Spark SQL.
- Build ML Model as per the requirement.
- Create entire data pipeline and ML pipe line
- Upload the entire project repository including source code to Github (you are required to create github account if you donâ€™t have it


# Architecture of the Pipeline:

## My SQL database creation using shell:

  a. Connect to the local instance of mysql using Command Line interface and create table schema.
  (Alternatively can be done using the source <tbl_creation_file.sql>
 
  b. LOAD data using the LOAD DATA INFILE command.

  c. Check schema associated with tables and validate data types and missing values loading, fix NULL if any.


## Uploading the SQL Database to HDFS and creating table in HIVE:

  a. USE a particular compressed format i.e. AVRO/ PARQUET/ ORC to transfer the data to HDFS, used PARQUET for this project.
    (Columnar structure gives fast querying support over AVRO row based structure)
   
  b. Create a Directory in the HDFS to bring in the files and avsc (schema files of the tables); used capstone/warehouse for the project.

  c. Transfer using the Scoop command, import all files in the capstone/warehouse dir using (import all comand); validate format and folders.


## Table creation in HIVE and Database Loading:

  a. Create tables in HIVE using the proper column_name and DATA TYPES (DATETYPES need to be imported as VARCHAR).

  b. IN Table definition add properties for the attribute TBLPROERTIES viz. file location of HDFS files and schema.

  c. Use proper delimiter to ensure right data loading ( Used ',' as CSV)

## Using HIVE and SPARK SQL to perform analysis:

  a. Create a Sql context object as entry point to do analysis, load the HIVE tables one by one to Dataframes.

  b. Data transfiormation i.e. converting DATE based column to DATETIME and removing "", other inconsistencies to query the data.

  c. Create tempviews to query the data.

## Spark ML model building:

  a. Import all the required libraries in Jupyter Labs

  b. Data preparation to clean out duplicate and null values.

  c. Proper formatting of table fields.

  d. Create categorical and continous features.

  e. Do Encoding on the categorical data and transform the data.

  f. Slit the dataset into train and test set.

  g. Perform Logistic Regression and Random Forest Classifier.

  h. Compare the result.
  
  
  # Analysis snippets:
  
  ### How the organization is segregated across different work_departments in terms of Gender, any skewness?
  
![gender_dep](https://user-images.githubusercontent.com/101033996/169641998-5e5fe231-097a-4008-a148-e242b425c0c7.png)


  ## Measuring the health of the organization:
  
  
  ## What is the employees avg mean tenure/ Duration
  
  ![Capture23](https://user-images.githubusercontent.com/101033996/169642204-749bfa19-c37e-4bcb-b9e9-7b4ee74d529e.PNG)
  
  ## What are resignation trends all these years cumulative:
  
  
  ![Capture34](https://user-images.githubusercontent.com/101033996/169642306-f7a3e9db-8085-49e1-9fcb-19e2e3ad4510.PNG)
  
  ## What are resignation trends across departments, is there any partcular department with large no of reginations witnessed?
  
  ![Capture45](https://user-images.githubusercontent.com/101033996/169642390-a28d352a-5c2c-46db-9b99-4ba31bb80124.PNG)
  
  
  
# Challenges faced During the Project:
-   Debugging the sql tables columns, and Datatypes, while importing the data from SQL to HIVE.
-   Importing the SQL tables used by using the SCOOP command, while some of the tables were missing PRIMARY KEYS, no partition was allowed
    (Used -m1 constraint to use a single mapper while importing tables in HIVE).
-   Creating ML models.


